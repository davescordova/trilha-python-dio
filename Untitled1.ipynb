{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxUQ+XyY5XBmjrK9rJCz3m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davescordova/trilha-python-dio/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "modYpV_CcTm2",
        "outputId": "af2deb74-b0a7-4e12-daca-8aaf7c81d5b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- [Célula 1] Carregando Importações ---\n",
            "--- Verificando GPU ---\n",
            "GPU device not found. Running on CPU.\n",
            "---------------------\n",
            "\n",
            "--- [Célula 2] Montando Google Drive ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2648454722.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- [Célula 2] Montando Google Drive ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Monta o Google Drive (irá pedir autorização)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Google Drive Montado.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# === INÍCIO DO CÓDIGO ÚNICO PARA GOOGLE COLAB ===\n",
        "# Este script executa um projeto de Transfer Learning de ponta a ponta.\n",
        "\n",
        "# --- Célula 1: Configuração e Importações ---\n",
        "print(\"--- [Célula 1] Carregando Importações ---\")\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Verifica se o Colab está usando GPU (essencial para Deep Learning)\n",
        "print(\"--- Verificando GPU ---\")\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print('GPU device not found. Running on CPU.')\n",
        "else:\n",
        "    print(f'Found GPU at: {device_name}')\n",
        "print(\"---------------------\\n\")\n",
        "\n",
        "\n",
        "# --- Célula 2: Montar Google Drive e Descompactar Dataset ---\n",
        "print(\"--- [Célula 2] Montando Google Drive ---\")\n",
        "# Monta o Google Drive (irá pedir autorização)\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive Montado.\")\n",
        "\n",
        "# --- DEFINA O CAMINHO DO SEU ZIP ---\n",
        "# !!! IMPORTANTE: Altere esta linha para o caminho do seu arquivo .zip no Google Drive !!!\n",
        "zip_path = '/content/drive/MyDrive/meu_dataset.zip'\n",
        "\n",
        "# --- DEFINA O CAMINHO DE EXTRAÇÃO ---\n",
        "extract_path = '/content/dataset'\n",
        "\n",
        "# Cria o diretório se não existir\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "# Descompacta o arquivo\n",
        "print(f\"--- Descompactando Dataset ---\")\n",
        "print(f'Extraindo {zip_path} para {extract_path}...')\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print('Extração concluída!')\n",
        "\n",
        "    # Define os caminhos para os diretórios de treino e validação\n",
        "    # Assumindo que o zip criou uma pasta 'meu_dataset' dentro de '/content/dataset'\n",
        "    base_dir = os.path.join(extract_path, 'meu_dataset')\n",
        "    train_dir = os.path.join(base_dir, 'train')\n",
        "    validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "    print(f\"Diretório de treino: {train_dir}\")\n",
        "    print(f\"Diretório de validação: {validation_dir}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRO: O arquivo {zip_path} não foi encontrado. Verifique o caminho.\")\n",
        "    print(\"O script não pode continuar sem o dataset.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro ao descompactar: {e}\")\n",
        "print(\"---------------------\\n\")\n",
        "\n",
        "\n",
        "# --- Célula 3: Preparar os Geradores de Imagem (Data Augmentation) ---\n",
        "print(\"--- [Célula 3] Preparando Geradores de Imagem ---\")\n",
        "# Parâmetros das imagens\n",
        "IMG_SIZE = (160, 160)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Cria o gerador de treino com Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# O gerador de validação não deve ter augmentation, apenas o pré-processamento\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Cria os geradores de fluxo a partir dos diretórios\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary' # 'binary' para 2 classes\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary' # 'binary' para 2 classes\n",
        "    )\n",
        "\n",
        "    # Pega os nomes das classes\n",
        "    class_names = list(train_generator.class_indices.keys())\n",
        "    print(f\"Classes encontradas: {class_names}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERRO: Não foi possível criar os geradores de dados. Verifique os caminhos e a estrutura das pastas.\")\n",
        "    print(f\"Erro detalhado: {e}\")\n",
        "    # Define os geradores como None para pular o treino\n",
        "    train_generator = None\n",
        "    validation_generator = None\n",
        "    class_names = []\n",
        "print(\"---------------------\\n\")\n",
        "\n",
        "\n",
        "# --- Célula 4: Construir o Modelo de Transfer Learning ---\n",
        "if train_generator:\n",
        "    print(\"--- [Célula 4] Construindo o Modelo ---\")\n",
        "    # Define o formato de entrada\n",
        "    IMG_SHAPE = IMG_SIZE + (3,)\n",
        "\n",
        "    # 1. Carregar o Modelo Base (MobileNetV2)\n",
        "    base_model = MobileNetV2(\n",
        "        input_shape=IMG_SHAPE,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    # 2. Congelar o Modelo Base\n",
        "    base_model.trainable = False\n",
        "    print(\"Modelo base (MobileNetV2) carregado e congelado.\")\n",
        "\n",
        "    # 3. Adicionar a \"Cabeça\" de Classificação\n",
        "    inputs = Input(shape=IMG_SHAPE)\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    # 1 neurônio e ativação 'sigmoid' para classificação binária.\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # 4. Criar o Modelo Final\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    # Compilar o modelo\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Mostra a arquitetura do modelo\n",
        "    model.summary()\n",
        "    print(\"---------------------\\n\")\n",
        "else:\n",
        "    print(\"--- [Célula 4] Pulando construção do modelo (dataset não encontrado) ---\")\n",
        "    model = None\n",
        "    history = None\n",
        "\n",
        "\n",
        "# --- Célula 5: Treinar o Modelo ---\n",
        "if model:\n",
        "    print(\"--- [Célula 5] Iniciando Treinamento Inicial ---\")\n",
        "    INITIAL_EPOCHS = 10\n",
        "\n",
        "    # Calcula os passos por epoch\n",
        "    steps_per_epoch = train_generator.n // BATCH_SIZE\n",
        "    validation_steps = validation_generator.n // BATCH_SIZE\n",
        "\n",
        "    print(f\"Iniciando o treinamento por {INITIAL_EPOCHS} epochs.\")\n",
        "\n",
        "    # Treina o modelo\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        epochs=INITIAL_EPOCHS,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_steps\n",
        "    )\n",
        "    print(\"---------------------\\n\")\n",
        "else:\n",
        "    print(\"--- [Célula 5] Pulando Treinamento Inicial ---\")\n",
        "\n",
        "\n",
        "# --- Célula 6: Visualizar os Resultados do Treinamento ---\n",
        "print(\"--- [Célula 6] Visualizando Resultados (Treino Inicial) ---\")\n",
        "\n",
        "def plot_history_simple(history_obj):\n",
        "    if history_obj is None or not hasattr(history_obj, 'history'):\n",
        "        print(\"Histórico de treino não disponível para plotagem.\")\n",
        "        return\n",
        "\n",
        "    acc = history_obj.history['accuracy']\n",
        "    val_acc = history_obj.history['val_accuracy']\n",
        "    loss = history_obj.history['loss']\n",
        "    val_loss = history_obj.history['val_loss']\n",
        "\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot de Acurácia\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, label='Acurácia (Treino)')\n",
        "    plt.plot(epochs, val_acc, label='Acurácia (Validação)')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Acurácia de Treino e Validação')\n",
        "\n",
        "    # Plot de Perda\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, label='Perda (Treino)')\n",
        "    plt.plot(epochs, val_loss, label='Perda (Validação)')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Perda de Treino e Validação')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "if 'history' in locals() and history:\n",
        "    plot_history_simple(history)\n",
        "else:\n",
        "    print(\"Sem histórico do treino inicial para plotar.\")\n",
        "print(\"---------------------\\n\")\n",
        "\n",
        "\n",
        "# --- Célula 7: (Opcional) Fine-Tuning ---\n",
        "if 'history' in locals() and history:\n",
        "    print(\"--- [Célula 7] Iniciando Fine-Tuning ---\")\n",
        "\n",
        "    # Descongela o modelo base\n",
        "    base_model.trainable = True\n",
        "    fine_tune_at = 100 # Descongelar da camada 100 em diante\n",
        "\n",
        "    # Congela todas as camadas *antes* da camada 'fine_tune_at'\n",
        "    for layer in base_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Re-compila o modelo com uma taxa de aprendizado (learning rate) muito baixa\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), # Taxa 10x menor\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    print(f\"Modelo re-compilado para Fine-Tuning. Camadas após {fine_tune_at} estão treináveis.\")\n",
        "    model.summary()\n",
        "\n",
        "    # Continua o treinamento (Fine-Tuning)\n",
        "    FINE_TUNE_EPOCHS = 10\n",
        "    total_epochs = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n",
        "\n",
        "    print(f\"Iniciando o Fine-Tuning por mais {FINE_TUNE_EPOCHS} epochs.\")\n",
        "    history_fine = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        epochs=total_epochs,\n",
        "        initial_epoch=history.epoch[-1], # Continua de onde parou\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_steps\n",
        "    )\n",
        "    print(\"---------------------\\n\")\n",
        "\n",
        "    print(\"--- Visualizando Resultados (Fine-Tuning) ---\")\n",
        "    plot_history_simple(history_fine)\n",
        "    print(\"---------------------\\n\")\n",
        "else:\n",
        "    print(\"--- [Célula 7] Pulando Fine-Tuning ---\")\n",
        "\n",
        "\n",
        "# --- Célula 8: Testar o Modelo com uma Nova Imagem ---\n",
        "if model:\n",
        "    print(\"--- [Célula 8] Teste com Nova Imagem ---\")\n",
        "    print(\"Faça upload de uma imagem para teste (o script continuará após o upload):\")\n",
        "    uploaded = files.upload() # Isso irá pausar e esperar pelo upload do usuário\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"Nenhum arquivo enviado. Fim do script.\")\n",
        "    else:\n",
        "        for fn in uploaded.keys():\n",
        "            # Carrega e processa a imagem\n",
        "            path = '/content/' + fn\n",
        "            img = image.load_img(path, target_size=IMG_SIZE)\n",
        "            img_array = image.img_to_array(img)\n",
        "            img_array = np.expand_dims(img_array, axis=0) # Cria um \"batch\" de 1 imagem\n",
        "\n",
        "            # Pré-processa a imagem\n",
        "            processed_img = preprocess_input(img_array)\n",
        "\n",
        "            # Faz a predição\n",
        "            prediction = model.predict(processed_img)[0][0] # Pega o valor da predição\n",
        "\n",
        "            # Mostra a imagem\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Interpreta a predição (sigmoid)\n",
        "            if class_names:\n",
        "                if prediction > 0.5:\n",
        "                    # O índice 1 é a classe_B\n",
        "                    print(f\"Previsão: {class_names[1]} (Confiança: {prediction*100:.2f}%)\")\n",
        "                else:\n",
        "                    # O índice 0 é a classe_A\n",
        "                    print(f\"Previsão: {class_names[0]} (Confiança: {(1-prediction)*100:.2f}%)\")\n",
        "            else:\n",
        "                 print(f\"Predição (bruta): {prediction:.4f}. (Nomes das classes não encontrados)\")\n",
        "\n",
        "            plt.show() # Mostra o gráfico da imagem\n",
        "            print(\"---------------------\\n\")\n",
        "else:\n",
        "    print(\"--- [Célula 8] Pulando Teste (modelo não foi treinado) ---\")\n",
        "\n",
        "print(\"=== FIM DO SCRIPT ===\")\n"
      ]
    }
  ]
}